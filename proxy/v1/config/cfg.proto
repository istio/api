// Copyright 2016 IBM Corporation
//
//   Licensed under the Apache License, Version 2.0 (the "License");
//   you may not use this file except in compliance with the License.
//   You may obtain a copy of the License at
//
//       http://www.apache.org/licenses/LICENSE-2.0
//
//   Unless required by applicable law or agreed to in writing, software
//   distributed under the License is distributed on an "AS IS" BASIS,
//   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
//   See the License for the specific language governing permissions and
//   limitations under the License.

syntax = "proto3";

import "google/protobuf/any.proto";
package istio.proxy.v1.config;

// Glossary & concepts

// Cluster - set of pods/VMs/containers where the service is running. The
// members of a cluster share one or more common attributes. For e.g., all
// pods in a cluster could share a common set of labels, or be running the
// same version of the application binary.

// Multiple clusters/service - In a continuous deployment scenario, for a
// given service, there can be multiple clusters running potentially
// different variants of the application binary. These variants are not
// necessarily different API versions. They could be iterative changes to
// the same service, deployed in different environments (prod, staging,
// dev, etc.). Common scenarios where this occurs include A/B testing,
// canary rollouts, etc. The choice of a particular cluster can be decided
// based on various criterion (headers, url, etc.) and/or by weights
// assigned to each cluster.

// One or more tags uniquely identify each cluster in a service. Tags can
// be arbitrary strings. Parsing of tags is specific to the platform (e.g.,
// in kubernetes, each tag string could be parsed into a pod label
// (key:value))

// Downstream service/cluster - client (browser or another service cluster)
// calling the proxy/sidecar (typically to reach another service).

// Upstream service - The remote service to which the proxy/sidecar is
// talking to, on behalf of the Downstream service. Upstream cluster refers
// to the cluster handling the request to the upstream service.

// Applications address only the upstream service without knowledge of
// individual upstream clusters. The actual choice of the cluster is
// determined by the proxy, enabling the application code to decouple
// itself from the evolution of dependent services.

// Proxy level global configurations go here
message ProxyConfig {
  // config specification version. defaults to 1.0
  string version = 1;

  // Traffic routing (http|tcp) related configurations for the
  // proxy/sidecar.  The configuration generated for the proxy will
  // preserve the order of rules specified by the user in the Istio config.
  // When a Http request matches multiple route rules, the first rule to
  // match will/should be chosen by proxy implementations N.B.: When a
  // request does not match any rule for Http, the downstream service would
  // receive a HTTP 404.
  repeated RouteRule route_rules = 2;
  
  // RouteRule determines the upstream cluster to which traffic should be
  // routed to.  The UpstreamClusterPolicy describes policies that
  // determine how to handle traffic (load balancing policies, failure
  // recovery policies such as timeouts, retries, circuit breakers), etc.
  repeated UpstreamClusterPolicy upstream_cluster_policies = 3;
}

// Describes rules for routing a request/connection to an upstream
// service, based on attributes associated with the request/connection
// and downstream service invoking the API call. The choice of the specific
// upstream cluster will be determined by the routing rule.
message RouteRule {
  oneof rule {
    Layer4RouteRule layer4_route_rule = 1;
    HttpRouteRule http_route_rule = 2;
  }
}

// Routes incoming Tcp/Udp traffic to one of the clusters of a service
// based on match criterion.
message Layer4RouteRule {
  // Set of conditions that must be satisfied, such as downstream cluster
  // labels, connection attributes, etc.
  Layer4MatchCondition match = 1;

  // Each routing rule is associated with one or more upstream clusters,
  // (see glossary in beginning of document). Weights associated with the
  // cluster determine the proportion of traffic it receives.
  repeated WeightedCluster weighted_clusters = 2;

  // Faults can be injected into the connections from downstream by the
  // proxy, for testing the failure recovery capabilities of downstream
  // services.  Faults include aborting the request/connection from
  // downstream service, delaying the proxying of request/connection to the
  // upstream service, and throttling the bandwidth of the connection
  // (either end).

  // TODO: This should be done on per-cluster basis.
  Layer4FaultInjection fault = 3;
}

// Routes Http requests to one of the upstream clusters based on
// match criterion.
message HttpRouteRule {
  // Set of conditions that must be satisfied, such as downstream service
  // labels, request attributes, etc.
  HttpMatchCondition match = 1;

  // TODO: add redirect support

  // Each routing rule is associated with one or more upstream clusters,
  // (see glossary in beginning of document). Weights associated with the
  // cluster determine the proportion of traffic it receives.
  repeated WeightedCluster weighted_clusters = 2;

  // Faults can be injected into the API calls by the proxy, for
  // testing the failure recovery capabilities of downstream services.
  // Faults include aborting the Http request from downstream service,
  // delaying the proxying of request to the upstream service, or both.
  // TODO: This should move into UpstreamCluster
  HttpFaultInjection fault = 3;

  // Other things? such as tracing, etc. ?
}

// Basic routing rule match criterion using Tcp attributes and downstream
// service tags
message Layer4MatchCondition {
  // Set of layer 4 match attributes such as src ip/port, dst ip/port and
  // protocol
  Layer4MatchAttributes layer4_attributes = 1;

  // Identify the downstream cluster initiating the connection.
  ClusterIdentifier src_cluster = 2;
}

// Layer4 connection match attributes
message Layer4MatchAttributes {
  // IPv4 or IPv6
  string src_ip = 1;
  // source port
  uint32 src_port = 2;
  // IPv4 or IPv6
  string dst_ip = 3;
  // destination port
  uint32 dst_port = 4;
  // protocol Tcp|Udp
  string protocol = 5;
}

// One or more tags that uniquely identify an upstream or downstream
// cluster. Tags can be arbitrary strings. The interpretation of tags is
// specific to the platform (e.g., kubernetes vs VMs)
message ClusterIdentifier {
  repeated string tags = 1;
}

message WeightedCluster {
  // Unique identity of the upstream cluster. Must be the same as those used
  // in the definition of the UpstreamClusterPolicy.
  ClusterIdentifier dst_cluster = 1;

  // The proportion of connections to be forwarded to the upstream
  // cluster. Max is 100. Sum of weights across versions should add up to
  // 100.
  uint32 weight = 2;
}

// TODO: Merge with the proposal for "Deriving Request Metadata in
// Kubernetes" by Sven Manson

// Http/1.1|Http/2|gRPC routing rule match criterion built on top of BaseMatchCondition
message HttpMatchCondition {
  // Set of layer 4 match attributes such as src ip/port, dst ip/port and
  // protocol
  Layer4MatchAttributes layer4_attributes = 1;

  // Identify the downstream cluster initiating the connection.
  ClusterIdentifier src_cluster = 2;

  // Set of Http request level match attributes
  string scheme = 3;

  oneof amatch {
    // exact match with the Http path (:path)
    string authority_match = 4;
    // prefix match on the path
    string authority_prefix = 5;
    // Posix style regular expressions. Throw error if the user's chosen
    // proxy does not support this.
    string authority_regex = 6;
  }

  oneof uri_match {
    // exact match with the Http path (:path)
    string path = 7;
    // prefix match on the path
    string prefix = 8;
    // Posix style regular expressions.  Throw error if the user's chosen
    // proxy does not support this.
    string regex = 9;
  }

  // Match Http requests based on the specified headers
  map<string, string> http_match_headers = 10;
}

// Describes the load balancing and failure recovery policies for a an
// upstream cluster. N.B. that these are enforced on egress
// connections/requests. i.e. enforced at the downstream service (caller).
message UpstreamClusterPolicy {
  // Unique identifier of the upstream cluster. The tags in the cluster
  // identifier are used by the service discovery component of the proxy to
  // identify the IP addresses of the pods|VMs running this service.
  ClusterIdentifier cluster = 1;

  // Should be either http://.. or tcp://.. 
  string health_check_endpoint = 2;
  LoadBalancingPolicy lb = 3;
  TimeoutPolicy timeout = 4;
  RetryPolicy retry = 5;
  CircuitBreakerPolicy circuit_breaker = 6;
}

// Load balancing policy to use when forwarding traffic to upstream clusters.
message LoadBalancingPolicy {
  enum SimpleLBPolicy {
    // These four simple load balancing policies have literally no
    // additional configuration.
    ROUND_ROBIN = 0;
    LEAST_CONN = 1;
    IP_HASH = 2;
    RANDOM = 3;
  }
  oneof lbpolicy {
    SimpleLBPolicy policy_name = 1;
    //Custom policy implementations
    google.protobuf.Any custom_lb_policy_impl = 2;
  }
}

// Request timeout: wait time until a response is received. Does not
// indicate the time for the entire response to arrive. 
message TimeoutPolicy {
  message SimpleTimeoutPolicy {
    // timeout is per attempt, when retries are specified as well.
    // seconds.nanoseconds format
    double timeout_seconds = 1;
    // Downstream service could specify timeout via Http header to the
    // proxy, if the proxy supports such a feature.
    string override_header_name = 2;
  }
  oneof timeoutpolicy {
    SimpleTimeoutPolicy simple_timeout = 1;
    // For proxies that support custom timeout policies
    google.protobuf.Any custom_timeout_policy_impl = 2;
  }
}

message RetryPolicy {
  message SimpleRetryPolicy {
    // number of times the request to the upstream cluster should be retried.
    // total timeout would be attempts * timeout
    uint32 attempts = 1;
    // Downstream Service could specify retry attempts via Http header to
    // the proxy, if the proxy supports such a feature.
    string override_header_name = 2;
  }
  oneof retrypolicy {
    SimpleRetryPolicy simple_retry = 1;
    // For proxies that support custom retry policies
    google.protobuf.Any custom_retry_policy_impl = 2;
  }
}

// A minimal circuit breaker configuration
message CircuitBreakerPolicy {
  message SimpleCircuitBreakerPolicy {
    // (for an unhealthy upstream cluster) number of consecutive requests that
    // should succeed before the upstream cluster is marked healthy.
    uint32 success_threshold = 1;

    // (for a healthy upstream cluster) number of consecutive requests that
    // can fail before the upstream cluster is marked unhealthy.
    uint32 failure_threshold = 2;

    // When a healthy upstream cluster becomes unhealthy, duration to wait before
    // attempting to send requests to that upstream cluster.
    // format seconds.nanoseconds
    double reset_timeout_seconds = 3;
  }
  oneof cbpolicy {
    SimpleCircuitBreakerPolicy simple_cb = 1;
    // For proxies that support custom circuit breaker policies.
    google.protobuf.Any custom_cb_policy_impl = 2;
  }
}

// Faults can be injected into the API calls by the proxy, for testing the
// failure recovery capabilities of downstream services.  Faults include
// aborting the Http request from downstream service, delaying the proxying
// of request to the upstream clusters, or both.
message HttpFaultInjection {
  // Delay requests before forwarding to upstream cluster, emulating
  // various failures such as network issues, overloaded upstream service, etc.
  Delay delay = 1;

  // Abort Http request attempts and return error codes back to downstream
  // service, giving the impression that the upstream service is faulty.
  // N.B. Both delay and abort can be specified simultaneously. In such
  // cases, a request would be first delayed and then aborted with an error
  // code.
  Abort abort = 2;
  
  // Only requests with these Http headers will be subjected to fault
  // injection
  map<string, string> http_match_headers = 3;

  // Either a fixed delay or exponential delay.
  message Delay {
    oneof http_delay_type {
      FixedDelay fixed_delay = 1;
      ExponentialDelay exp_delay = 2;
    }
  }

  // Add a fixed delay before forwarding the request to upstream cluster
  message FixedDelay {
    // percentage of requests on which the delay will be injected
    float percent = 1;
    // delay duration in seconds.nanoseconds
    double fixed_delay_seconds = 2;
    // Specify delay duration as part of Http request
    string override_header_name = 3;
  }

  // Add a delay (based on an exponential function) before forwarding the
  // request to upstream cluster
  message ExponentialDelay {
    // percentage of requests on which the delay will be injected
    float percent = 1;
    // mean delay needed to derive the exponential delay values
    double mean_delay_seconds = 2;
  }

  // Abort Http request attempts and return error codes back to downstream
  // service.
  message Abort {
    // percentage of requests to be aborted with the error code provided.
    float percent = 1;
    // Error code to use to abort the Http request. Requests can be aborted
    // either with Http/1.1 status codes | http2 error codes or gRPC status
    // codes.
    oneof error_type {
      string grpc_status = 2;
      string http2_error = 3;
      uint32 http_status = 4;
    }
    // Specify abort code as part of Http request
    string override_header_name = 5;
  }
}

// Faults can be injected into the Layer4 traffic forwarded by the proxy, for
// testing the failure recovery capabilities of downstream services.
// Faults include terminating established Tcp connections, throttling the
// upstream/downstream bandwidth (for Tcp|Udp), or both.
message Layer4FaultInjection {
  // Unlike Http services, we have very little context for raw Tcp|Udp
  // connections. We could throttle bandwidth of the connections (slow down
  // the connection) and/or abruptly reset the Tcp connection after it has
  // been established.
  Throttle throttle = 1;
  Terminate terminate = 2;

 // Bandwidth throttling for Tcp and Udp connections
  message Throttle {
    // percentage of connections to throttle.
    float percent = 1;
    // bandwidth limit in "bits" per second between downstream and proxy
    uint64 downstream_limit_bps = 2;
    // bandwidth limits in "bits" per second between proxy and upstream
    uint64 upstream_limit_bps = 3;

    oneof throttle_after {
      // Wait for X seconds after the connection is established, before
      // starting bandwidth throttling. This would allow us to inject fault
      // after the application protocol (e.g., MySQL) has had time to
      // establish sessions/whatever handshake necessary.
      double throttle_after_seconds = 4;

      // Alternatively, we could wait for a certain number of bytes to be
      // transferred to upstream before throttling the bandwidth.
      double throttle_after_bytes = 5;
    }

    // Stop throttling after the given duration. Set to 0 to throttle the
    // connection for its lifetime.
    double throttle_for_seconds = 6;
  }

  message Terminate {
    // percentage of established Tcp connections to be terminated/reset
    float percent = 1;
  }
}
